+++
title = 'Paper Reading: Zero-Shot Cost Models for Out-of-the-box Learned Cost Prediction [VLDB 2022]'
date = 2024-03-16T09:58:30-04:00
draft = false
tags = ['Paper Reading']
+++

## Zero-Shot Cost Models for Out-of-the-box Learned Cost Prediction [VLDB 2022]

## Abstract

æœ¬æ–‡ä»‹ç»äº† zero-shot cost modelï¼Œè¯¥æ¨¡åž‹å¯ä»¥ä½¿å­¦ä¹ çš„æˆæœ¬ä¼°ç®—èƒ½å¤Ÿ generalizes to unseen databasesã€‚ä¸Žæœ€ state-of-the-art çš„å·¥ä½œè´Ÿè½½é©±åŠ¨çš„æ–¹æ³•ç›¸åï¼Œè¿™äº›æ–¹æ³•ä¸å¿…åœ¨æ¯ä¸ªæ–°æ•°æ®åŸºç¡€ä¸Šæ‰§è¡Œå¤§é‡ training queries ï¼Œzero-shot cost models thus allow to instantiate a learned cost model out-of-the-box **without expensive training data collection**ã€‚ä¸ºäº† zero-shot cost modelsï¼Œæœ¬æ–‡æå‡º a new learning paradigm based on **pre-trained cost models**ã€‚As core contributions to support the transfer of such a **pre-trained cost model** to **unseen databases**, we introduce a new model architecture and **representation technique for encoding query workloads as input** to those models. As we will show in our evaluation, zero-shot cost estimation can **provide more accurate cost estimates** than state-of-the-art models for a wide range of (real-world) databases without requiring any query executions on **unseen databases**. Furthermore, we show that zero-shot cost models can be used in a few-shot mode that further improves their quality by retraining them just with a small number of additional training queries on the unseen database.

> ä»¥åŽå°½å¯èƒ½å°‘åœ°ç›´æŽ¥æœºç¿»åŽŸè®ºæ–‡äº†ï¼Œå¤ªå¤šåè¯æœºç¿»æ•ˆæžœå¾ˆå·®ï¼Œåè€Œé™ä½Žé˜…è¯»é€Ÿåº¦ã€‚åŸºæœ¬ä¸Šè¯»å®Œå°±å†™ä¸€ç‚¹æ„Ÿæƒ³ã€‚

## Introduction

Motivation: **Accurate physical cost estimation** (i.e., estimating query latencies) is crucial for **query optimization** in DBMSs. Classically, cost estimation is performed using models that make several simplifying assumptions. As a result, such models often over or underestimate runtimes, leading to **suboptimal planning decisions** that degrade the overall query performance. Recently, machine learning has thus been used for learned cost models that do not need to make such simplifying assumptions.

While it was shown that the **cost estimates** of such learned cost models are significantly **more accurate** than those of the traditional cost models, the existing approaches rely on **workload-driven learning** where models have to observe thousands of queries on the same database for which the cost prediction should be performed. This workload execution is required to gather the training data which can take hours (or days) since tens of thousands of queries need to be executed on potentially large databases

In Figure 1, we show the **cost estimation accuracy** depending on how many hours we allow for gathering the **training data for a workload-driven model**. As we can see, even for a medium-sized database such as IMDB, it takes more than 5 hours of running queries on this database to gather enough training data such that the cost estimation model can provide a decent accuracy.

Unfortunately, **collecting training data** by running queries is not a one-time effort. In fact, the training data collection has to be repeated for every new database a learned model should be deployed for. This is due to the fact that current model architectures for workload-driven learning tie a **trained model to a particular database instance**. Consequently, **for every (new) unseen database** we not only have to train a model from scratch but also gather training data in the form of queries. And even for the same database, in case of **changed data characteristics** due to updates, training data collection needs to be repeated. Overall, these repeated high costs for obtaining training data for unseen databases render workloaddriven learning unattractive for many practical deployments.

> è¿™é‡Œå‡ ä¸ªåè¯å¾ˆå¥‡æ€ª unseen database å’Œä»– database çš„æ¦‚å¿µä¹Ÿä¸åŒï¼Œå®žé™…ä¸ŠæŒ‡çš„æ˜¯å…·æœ‰ç‰¹å®šæ•°æ®ç‰¹ç‚¹çš„æ•°æ®é›†ã€‚
>
> å¯¹äºŽ query optimization éœ€è¦æ”¶é›†å¾ˆå¤šæ•°æ®é›†ï¼Œå¹¶ä¸”ç²¾è¯»ä¸å¤Ÿé«˜ï¼Œè€Œä¸”å¯¹äºŽæ–°çš„æ•°æ®é›†åˆè¦é‡æ–°æ”¶é›†ã€‚zero-shot å¸Œæœ›èƒ½ç”¨ä¸€ç§è®­ç»ƒæ¨¡åž‹æ¥æ³›åŒ–æ•°æ®é›†ã€‚

![](https://s2.loli.net/2024/03/16/QqEyBkIbfGhPSmr.png)

Contributions: In this paper, we thus suggest **a new learning paradigm for cost estimation called zero-shot cost models** that reduces these high efforts. The general idea behind zero-shot cost models is motivated by recent advances in **transfer learning of models**. Similar to other approaches such as **GPT-3** which **enable zero-shot learning for NLP**, a zero-shot cost model is trained on a wide collection of different databases and workloads and can thus **generalize out-of-the-box** to a completely **unseen database** without the need to be trained particularly on that database. In fact as depicted in Figure 1, zero-shot cost models can provide a high accuracy and often even outperform existing workload-driven approaches that have been trained on large sets of training queries. Moreover, as we also show in Figure 1, zero-shot cost models can additionally be **fine-tuned on the unseen database** with just a few training queries and the resulting few-shot models further improve the accuracy.

> æœ¬æ–‡çš„æƒ³æ³•æ˜¯ç±»ä¼¼è¯­è¨€æ¨¡åž‹è®­ç»ƒï¼Œè¿›è¡Œæ— æ ‡æ³¨çš„å¤§é‡é¢„å…ˆè®­ç»ƒï¼Œæ‰€ä»¥å¯ä»¥å¯¹æ²¡è§è¿‡çš„æ•°æ®é›†ç›´æŽ¥è¿›è¡Œä»£ä»·ä¼°è®¡ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®çŽ‡ï¼Œç”šè‡³æ¯” workload-driven çš„è¡¨çŽ°å¥½ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ fine-tunedã€‚

One could now argue that it might be a significant effort to **collect sufficient training data across databases for pre-training a zero-shot model**. However, in contrast to workload-driven models which require training data for every unseen database, training data collection is a **one-time effort**; i.e., once trained the zero shot model can be used for any new unseen database. In fact, in our evaluation we show that zero-shot models can provide high accuracies for a **wide-variety of real-world databases**. Moreover, for historical traces can be used which eliminates the need to collect any training data. For example, cloud providers such as AWS, Microsoft, or Google, typically anyway keep logs of their customer workloads which could directly be used as training data for zero-shot learning without collecting any further training data

A key aspect to enable zero-shot learning is that **a cost model can be transferred to new (unseen) databases**, i.e., the models leverage observed query executions on a variety of different databases to **predict** runtimes on new (unseen) databases. However, state-of-the-art model architectures used for workload-driven learning do not support this training and inference mode since they are tied to a **particular database**. As a core novel contribution for zero-shot cost models we thus devise a new model architecture based on a representation of queries that generalizes across databases using a **transferable representation** with features such as the tuple width that can be derived from any database. Moreover, zero-shot models **separate concerns**; i.e., data characteristics of a new database (e.g., rows of tables) are not implicitly learned as in classical workload-driven learning (which hinders generalization), but are provided as input to the model

> separate concerns æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿæ–°æ•°æ®é›†çš„ç‰¹å¾ä¸åŒï¼Ÿå’Œä¸å¤Ÿè¶³å¤Ÿé«˜çš„ç²¾åº¦ï¼Ÿ

Another core question for zero-shot models is at which point a **sufficient** amount of different training databases and workloads was observed to generalize robustly to unseen databases. To answer this question, as a second contribution in this paper we derive a method to **estimate how accurate the runtime estimations of zero-shot models** will be for unseen databases. We also discuss how to address cases of workload drifts where the zero-shot models are expected to generalize less robustly. Furthermore, we also show that zero-shot models are widely applicable beyond cost models for **query optimizers** for single-node DBMSs which is the main focus of this paper. For instance, we believe that zero-shot cost models can be **naturally extended to to distributed DBMS** or even other use cases such as providing cost estimates for design advisors where the goal is to automatically find a suitable database design (e.g., a set of indexes) for a given workload.

Finally, in our extensive experimental evaluation, we verify that zero-shot cost models **generalize robustly to unseen databases and workloads** while providing **cost estimates** which are more accurate than those of workload-driven models. As part of this evaluation, we also provide a new **benchmark** (beyond JOB) which is necessary to evaluate cost estimation models more broadly on a variety of (real-world) databases. We will make this benchmark including query executions for training cost models publicly available and hope that it will benefit future research in learned cost estimation and potentially beyond.

> æ²¡æŽ¥è§¦è¿‡è¿™æ–¹å‘çš„å·¥ä½œï¼Œå¥½åƒæ˜¯ç”¨æ•°æ®é›†åšä¸€äº›æ¯”è¾ƒæ³›åŒ–çš„å·¥ä½œ

## Overview

### Problem Statement

> ä¸»è¦ç›®æ ‡æ˜¯é¢„æµ‹åœ¨ unseen database ä¸Šçš„ query latencyï¼Œå¹¶ä¸”æ²¡æœ‰æå‰çŸ¥é“ä»»ä½• queryã€‚
>
> æ–‡ç« è¯´ zero-short cost estimation å’Œ stoa workload-driven cost model æœ‰å¼ºçƒˆçš„å¯¹æ¯”ã€‚å¹¶ä¸”å¸Œæœ›èƒ½å¤Ÿåœ¨æµæ•°æ®åº“ã€å›¾æ•°æ®åº“åšä¸€äº›æœªæ¥çš„å·¥ä½œã€‚

Finally, while we believe that zero-shot learning for DBMSs is more **generally applicable**, we restrict ourselves in this paper to **cost estimations** for **relational DBMSs** (both single-node and distributed). In particular, zero-shot cost models for other types of systems such as graph-databases or streaming systems are interesting avenues of future work.

### Our Approach

A key challenge for developing zero-shot cost models is the question how to design a model that allows to **generalize across databases**.

Typically, these consist of two models: **a database-agnostic model** to estimate the runtime cost and a database dependent model (e.g., histograms) to capture data characteristics. When predicting the cost of a query, the estimated cardinalities and other characteristics (i.e., outputs of the database-dependent models) serve as input to the general database-agnostic cost model which captures the general system behavior (e.g., the costs of a sequential scan grows linearly w.r.t. the number of rows). While the classical models are lightweight, they often largely under or overestimates the true costs of a query since models are too simple to capture complex interactions in the query plan and data.

> classical cost model åŒ…å«ä¸¤ä¸ª modelï¼Œä¼°è®¡è¿è¡Œæ—¶ cost å’Œæ•èŽ· modelï¼Ÿ

è¿™ä¸ªæ¨¡åž‹åˆ†å¼€äº†å…³æ³¨ç‚¹ï¼Œç”¨ä¸€ä¸ªæ›´åŠ ä¸°å¯Œçš„å…·æœ‰çŸ¥è¯†çš„ learned æ¨¡åž‹ï¼Œ similarly takes data characteristics of the unseen database as input to predict query runtimes in a database-agnostic mannerã€‚ç”¨ä¸åŒçš„ query plan å’Œ data characteristic of the plan è¿›è¡Œè®­ç»ƒã€‚

to predict the runtime of a query plan on a new (unseen) database, we feed the query plan together with its **data characteristics** into a zero-shot model.

> data characteristics æ˜¯ä»€ä¹ˆï¼Ÿtuple width, intermediate cardinalitiesï¼Œæ‰€ä»¥éœ€è¦æ•°æ®é©±åŠ¨å­¦ä¹ ï¼Ÿä½†è¿™æ˜¯å¦è¿åäº† zero-shotï¼Ÿè¿˜æ˜¯è¯´åªéœ€è¦è®­ç»ƒæ•°æ®ç‰¹å¾è€Œä¸æ˜¯ query

![](https://s2.loli.net/2024/03/17/HzPkYLps5lj1FZo.png)

> è®­ç»ƒç„¶åŽæŽ¨ç†ï¼Œå’Œè¯­è¨€æ¨¡åž‹ç±»ä¼¼
>
> ä½†æ˜¯æ€Žä¹ˆ estimate runtime of a plan given its data characteristics?

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ query è¡¨ç¤ºæ–¹æ³•ï¼Œæ—§çš„ä¸æ˜¯ transferable ä¹Ÿä¸æ˜¯ zero-shotã€‚è¿™ç§è¡¨çŽ°æ–¹å¼ completely relies on features that can be derived from any database to allow the model to generalize to unseen databases. For example, predicates for **filter operations** in a query are encoded by the general predicate structure (e.g., which data types and comparison operators are used in a predicate) instead of encoding the literals. In addition, data characteristics of a filter operator (e.g., input and output cardinality to express the selectivity) are provided as additional input to a zero-shot model. That way, a zero-shot model can learn the runtime overhead of a filter operation based on database-agnostic characteristics. We present details of our query representation in Section 3.

Finally, a last important aspect of zero-shot cost models is that they can easily be extended to **few-shot learning**. Hence, instead of using the zero-shot model out-of-the box (which already can provide good performance), one can **fine-tune** the model with only a few training queries on an unseen database

## Zero-Shot Cost Models

1. how we devised such a **transferable** query representation
2. how **inference and training** of a zero-shot model that uses this representation works

### Query Representation

State-of-the-art workload-driven models for cost estimation do not use **transferable** query representations and can thus only be used on the database they were trained on

1. Query Representation for Workload-Driven Models: hard-code the model against a single database, **attribute names** (e.g., those used in filter predicates) are typically encoded using a one-hot encoding assigning each attribute present in the database a specific position in a feature vector. For instance, the attribute `production_year` of the IMDB dataset might be encoded using the vector (0,1,0)(assuming that there are only three attributes in total). If the same model should now be used to predict query costs for the SSB dataset, some attributes might not even exist or even worse they might exist but have very different data distributions or even a different data type. In fact, non-transferable feature encodings are not only used for **attributes** but in various places of the query representation such as encoding table names or literals in filter predicates.

2. Query Representation for Zero-Shot Cost Models: Hence, for **zero-shot cost models** we require a new query representation that is **transferable** across databases. The main idea of the **transferable representation** we suggest in this paper is shown in Figure 3. At the core, a **query plan** and the **involved tables and attributes** are represented using a **graph** where graph nodes use **transferable features** 1 (i.e., features that provide meaningful information to **predict runtime** on different databases). This representation then serves as input for the training and inference process of zero shot cost models 2 ~ 4 that we explain in the **subsequent sections**. In the following, we discuss the **graph encoding** of the transferable featurization in detail

3. Graph Encoding of Query Plans: While **graph-based** representations have been already used to represent query operators of a query plan [28], our representation has **significant differences**. First, as shown in Figure 3 1 , our representation not only **encodes physical plan operators** as nodes (gray) in the graph as in previous work [28], but it also **covers all query plan information more holistically** using different nodes types for **attributes** (green), **tables** (blue) as well as predicate information (red). Second, as discussed before, previous approaches also covered such information, however, they used one-hot-encodings (which are non-transferable) while our representation captures the query complexity in a transferable way

For instance, to encode filter predicates, different from previous approaches we encode the predicate structure as nodes (red) without literals. In particular, we encode information such as data types of the attributes and operators used for comparisons. For example, the filter predicate company_type_id=2 for the query 0 in Figure 3, is encoded using an attribute node (ð‘¥5) with the comparison node = (ð‘¥7). As such, a zero-shot cost model provided with the transferable features (e.g., intermediate cardinalities which are given by the data-driven models) can infer the complexity of the predicates to estimate the query runtime

![](https://s2.loli.net/2024/03/17/pEd3QMsvJArb4ZO.png)

Transferable Featurization: nodes in the graph 1 (e.g., plan operators as shown in gray) are **transferable**. In particular, when used on **different databases**, features should not encode any implicit information that hinder the **transfer of the model to a new unseen database**. The set of such features used for the different node types in our query representation is depicted in Table 1. For instance, attribute nodes (green) use features such as the **data type** or the **width** in bytes. Similarly, for tables (blue nodes), we use other **transferable features** (e.g., the **number of rows** as well as the **number of pages on disk**)

transferable features can either characterize the query plan (e.g., operator types) or represent the data characteristics (e.g., intermediate cardinalities)

For transferable features that **represent data characteristics** many can be derived from the **metadata** of a database (such as the the **number of rows** of a table node). However, some other features that represent data characteristics â€” e.g., the **estimated output cardinality** of an operator node â€” require more involved techniques. In Section 3.4, we discuss alternatives of how we provide **estimated output cardinalities to zero-shot cost models.**

> è¿˜æ˜¯æ²¡ç†è§£ Transferable feature æ˜¯ä»€ä¹ˆï¼Œæ˜¯èƒ½ä»£è¡¨ä¸€ä¸ªæ•°æ®çš„ç‰¹å¾å’Œ query plan å—ï¼Ÿintermediate cardinalities ä¹Ÿä¸çŸ¥é“æ˜¯ä»€ä¹ˆï¼Œæ˜¯ join cardinality å—ï¼Ÿä¼ ç»Ÿæ˜¯åŸºäºŽç›´æ–¹å›¾ï¼Œä¹‹å‰æ•°æ®åº“è¯¾ä¸Šä¹Ÿè§è¿‡ä¸€äº›ï¼Œç”¨ç›´æ–¹å›¾åšä¸€äº›ç»Ÿè®¡å’Œä¼°è®¡ã€‚

### Inference on Zero-Shot Models

ç”¨äº†æ¯ä¸ªå›¾èŠ‚ç‚¹çš„ç‰¹å¾å‘é‡æ¥åš MLP å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆæœºå™¨å­¦ä¹ ï¼‰ï¼ŒåŠ ä¸Š GNN åšæ¶ˆæ¯ä¼ é€’ï¼Œç”¨ DAG è¡¨ç¤ºæŸ¥è¯¢

We thus use a novel **bottom-up message passing scheme** through the graph (i.e., in topological ordering) to obtain an updated hidden state â„Žâ€²ð‘£ of a node ð‘£ that contains all information of the child nodes.

> æœºå™¨å­¦ä¹ å†…å®¹ï¼Œæ²¡å¤ªç†è§£

### Training Zero-Shot Models

åœ¨å‡ ä¸ªæ•°æ®é›†å’ŒæŸ¥è¯¢é›†ä¸Šåšäº†è®­ç»ƒ

### Deriving Data Characteristics

zero shot model å’Œå•ä¸ªæ•°æ®é›†çš„åˆ†å¸ƒæ— å…³ï¼Œæ‰€ä»¥èƒ½é¢„æµ‹ unseen datasetï¼ŒåŒæ—¶ä¸ºäº†å®žçŽ°è¯¥ç‰¹æ€§ï¼Œéœ€è¦æä¾›æ•°æ®çš„ç‰¹å¾ï¼ˆå±žæ€§å®½åº¦ã€é¡µé¢æ•°é‡ã€è¡¨æ ¼è¡Œæ•°é‡ï¼‰

> æ‰€ä»¥éœ€è¦äººä¸º labelï¼Ÿæ²¡æœ‰è§£é‡Šæ¸…æ¥šåˆ°åº•æ€Žä¹ˆé¢„æµ‹ cardinality estimation
>
> æœ¬æ¨¡åž‹æ˜¯å’Œ traditional approach å’Œ data-driven model è¿›è¡Œæ¯”è¾ƒï¼Œä½† zero-shot æœ€å¥½å¯ä»¥å’ŒåŽè€…ç»“åˆï¼Œä¼ ç»Ÿæ–¹æ³•ä½œä¸º fallbackã€‚

In our evaluation, we see that zero-shot models can still produce **reasonable estimates** even if only **cardinalities estimates** from **traditional models** are available

## Robustness of Zero-Shot Models

é’ˆå¯¹ unseen datasets, zero-shot model å…·æœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨ä»€ä¹ˆæ—¶å€™å¯ä»¥è§‚å¯Ÿåˆ°è¶³å¤Ÿå¤šçš„ä¸åŒè®­ç»ƒæ•°æ®åº“(å’Œå·¥ä½œè´Ÿè½½) ï¼Œä»Žè€Œå¯ä»¥é²æ£’åœ°æŽ¨å¹¿åˆ° unseenï¼Ÿ

é›¶æ‹æ¨¡åž‹çš„è¿è¡Œæ—¶ä¼°è®¡çš„é²æ£’æ€§ï¼ŒWe then discuss a simple method to detect cases of workload drifts (i.e., the queries at runtime have substantially different characteristics than the training queries) as well as strategies how to tackle this problem.

### Estimating the Generalization Performance

1. simply esimate the generalization errors
2. (actually use) estimate if **additional training databases** will improve the generalization performance

æ‰€ä»¥è®­ç»ƒæ—¶ç”¨äº† subsetsï¼Œç„¶åŽç”¨æ›´å¤§çš„ train set å¦‚æžœæ²¡æœ‰æ”¹è¿›ï¼Œåˆ™æ— æ³•æé«˜ generalization èƒ½åŠ›

### Tackling Workload Drifts

new database and workload, we suggest a strategy to **detect cases of workload drifts** by monitoring the test error and propose to **tackle workload-drifts** using few-shot learning

> è¿™æ˜¯ä¸€ä¸ªè›®æ–°å¥‡çš„ ideaï¼Œç”¨å¾®è°ƒè¿‡çš„æ¨¡åž‹æ¥æ£€æµ‹å’Œè§£å†³ workload drifts

## Extensions of Zero-Shot Cost Models

zero-shot cost estimation can be **extended in various directions**.

### Distributed DBMSs

æœ¬æ–‡çš„ç ”ç©¶å¯¹è±¡è¿˜æ˜¯ å•æœº DBMSï¼Œè¿™ä¸€èŠ‚è®¨è®ºäº†å¦‚ä½•æ”¯æŒåˆ†å¸ƒå¼ DBMSï¼Œå°¤å…¶æ˜¯å½“æŸ¥è¯¢æ‰§è¡Œæ—¶æ•°æ®ä¼šè¢«æ‰“ä¹± shuffleã€‚å¹¶ä¸”è®¨è®ºäº†ä¸€äº›åˆ—å­˜/è¡Œå­˜æ—¶ï¼ŒæŸ¥è¯¢ä¼˜åŒ–çš„åŒºåˆ«ã€‚

ç„¶åŽæœ¬æ–‡æå‡ºäº†å¯¹ æŸ¥è¯¢ è¿›è¡Œæ‰©å±•ç¼–ç ï¼ŒåŒ…å« **operator nodes** for data shuffling as well as **encode data formats** (column or row) as a feature of the table node

### Physical Design Tuning

æ²¡æ‡‚

### Other Directions

hardware parameters, database knob configurations...

## A New Benchmark

æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ benchmark

### Design Decisions

æœ¬æ–‡è®¤ä¸ºä¼ ç»Ÿ TPC-H TPC-DS or SSB ä¸è¶³å¤Ÿè¯„ä¼°æˆæœ¬é¢„æµ‹æ¨¡åž‹ï¼Œå¹¶ä¸”å› ä¸ºè¿™äº›æ•°æ®æ˜¯åˆæˆçš„ï¼Œæ²¡æ³•æ•æ‰åˆ°ç›¸å…³æ€§æ¥æä¾› cardinality estimationï¼Œä¹Ÿæœ‰äººåŸºäºŽ IMDB æ•°æ®é›†ç»„äº†ä¸€äº›æ–°çš„ workload ç—…æ•…å­¦ä¹ çš„æˆæœ¬å’Œ cardinality estimationã€‚ä½†è¿™ä¹Ÿä¸èƒ½è¯„ä¼° zero-shotï¼Œå› ä¸ºè¦åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œéœ€è¦ä¸€ä¸ªè·¨è¶Šå¤šç§æ•°æ®é›†çš„ benchmarkã€‚

### Datasets

correlations hardly resemble data distributions found in the real-world

æœ¬æ–‡ä½¿ç”¨äº†çœŸå®žä¸–ç•Œæ•°æ®é›†ï¼Œä¹ŸåŠ å…¥äº† SSB å’Œ TPC-H

### Workloads and Traces

20 ä¸ªæ•°æ®é›† + ç”Ÿæˆçš„æŸ¥è¯¢

## Experimental Evaluation

![](https://s2.loli.net/2024/03/17/NeI2ahUsbTGYg1P.png)

1. Zero-Shot Accuracy, predict costs for unseen databases.
2. Zero-Shot vs. Workload-Driven, compare the training overhead and accuracy with state-of-the-art workload driven approaches
3. Generalization, workload drifts
4. Extensions, distributed DBMSs and different physical designs
5. Training and Inference Performance, compare training efforts to workload-driven models
6. Ablation Study, alternatives of zero-shot models, how many database are sufficient for zero-shot cost models to generalize

> å®žéªŒç»“æžœå°±ä¸çœ‹äº†ï¼Œæ–‡ç« æ²¡æœ‰æ·±å…¥è®¨è®ºä»–ä»¬çš„æ•°æ®é›†çš„åŒºåˆ«ï¼Œå®žéªŒç»“æžœçœ‹èµ·æ¥å’Œ workload-driven çœ‹ä¸ŠåŽ»ä¹Ÿå·®ä¸å¤š

## Conclusion

> The paper proposes a novel zero-shot cost model that enables learned cost estimation to unseen databases without extensive training on new datasets. The architecture of zero-shot cost model is very similar to the language model in NLP. It adopts the idea of â€‹â€‹pre-training, so that it can perform well on different datasets, and can also be fine-tuned for special datasets. The paper details the methods used to enable zero-shot cost models, including the transferable query representation, training zero-shot models and deriving data characteristics.The paper also discusses the robustness and scalability of zero-shot models (distributed database). The paper proposes a new benchmark spanning multiple datasets, and gives experimental results, proving that zero-shot models can perform excellent cost estimation on unseen datasets

> 1. The paper proposes a novel zero-shot cost model, and suggests a learning paradigm based on pre-trained cost models. A new model architecture and a representation technique for encoding query workloads as input to these models are introduced, which together allow a zero shot cost model to generalize to an unseen database.
> 2. The paper derives a method to estimate how accurate the runtime estimations of zero-shot models will be for unseen databases, and provides a new benchmark necessary to evaluate cost estimation models more broadly on a variety of real-world databases.
> 3. The paper not only discusses the zero-shot cost model based on a single-node DBMS, but also discusses future work, proving that zero-shot cost estimation can also be achieved in a distributed database. It can also be combined with data-driven models or even fine-tuned zero-shot models for better cost estimations and accuracy.

> 1. The zero-shot cost model may not always accurately predict costs for queries on databases that significantly differ from those seen during the modelâ€™s training, because it relies on pre-trained models and there are often large differences between real-world datasets and query workloads.
> 2. This paper somewhat confuses the concepts of datasets and databases. And while zero-shot models can eliminate the need for extensive training on each new database, they may still require some fine-tuning or a few-shot learning approach to achieve the best performance on specific datasets and queries, which is not completely zero-shot.
> 3. Although the paper has used 20 widely different data sets for benchmarking, 19 of them were used for training and only 1 was used for testing. It should be tested with more data sets.

> First, I would try to create a larger and more diverse training and testing dataset that covers a wide range of database schemas and query workloads and extend the benchmark used for evaluating cost models to include more real-world databases and complex queries. This would provide a more comprehensive assessment of the modelâ€™s performance and its ability to generalize. Second, extending the zero-shot model to distributed databases or more databases seems very feasible. I might try to expand this part a little further, especially to implement zero-shot cost models fordistributed stream processing or graph databases, because the queries and datasets for stream processing and graph database will be very different. The other parts of the paper are very perfect, especially the architecture that draws on language models, which is very clever.

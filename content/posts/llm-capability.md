+++
title = 'Paper Reading: DB-BERT: a Database Tuning Tool that “Reads the Manual”'
date = 2024-02-04T12:38:04-05:00
draft = false
tags = ['Paper Reading']
+++

## DB-BERT: a databse tuning tool that reads the manual

DB-BERT，一个读了手册的数据库调优工具

> https://arxiv.org/pdf/2112.10925.pdf

## Abstract

DB-BERT 是一种**数据库调优工具**，可利用通过对手册和其他相关文本文档进行**自然语言分析**而获得的信息。它利用文本来识别需要调整的数据库系统参数以及推荐的参数值。DB-BERT 应用预先训练好的大型语言模型（特别是 **BERT 模型**）进行文本分析。在初始训练阶段，它会**微调模型权重**，以便将自然语言提示转化为推荐设置。在运行时，DB-BERT 会学习汇总、调整和优先处理提示，以**实现特定数据库系统和基准的最佳性能**。这两个阶段都是**迭代式**的，并使用**强化学习**来指导选择要评估的调整设置（惩罚数据库系统拒绝接受的设置，同时奖励提高性能的设置）。在实验中，我们利用数百篇有关数据基础调整的文本文档作为 DB-BERT 的输入。我们将 DB-BERT 与不同的基准（TPC-C 和 TPC-H）、指标（吞吐量和运行时间）以及数据库系统（Postgres 和 MySQL）进行了比较。在所有情况下，DB-BERT 都能在所有比较方法中找到**最佳参数设置**。源码：https://itrummer.github.io/dbbert/

## Introduction

> Give me a user manual, and I’m happy for hours. -- Lennon Parham
>
> When all else fails, read the instructions. -- Anonymous

手册是很有用的，例如，在开始调优数据库管理系统（DBMS）之前，建议先阅读相关手册。但一直以来这些手册似乎只适用于人类，而无法被机器阅读，但是随着基于 Transformer 架构的，强大的预训练语言模型的出现，情况发生了改变。本文 的 DB-BERT 就基于 BERT 模型，通过阅读（自然语言分析）数据库手册和数百份关于调优的文本，来寻找更有希望的数据库系统参数进行调优。

近年来，为特定工作负载和性能指标寻找 **DBMS 参数**（也称 tuning knobs）的最佳参数值的问题受到了广泛关注。现在的 DBMS 有数百个参数，因此很难手动找到最佳设置。这就激发了**自动参数调整**的计算方法。目前的主流方法是机器学习，尤其是**强化学习**。在这种方法中，调优工具会根据特定设置的基准运行结果，有原则地选择要尝试的数据库管理系统参数值组合。然而，这种方法**成本高昂**（最近的工作在每次调整过程中都要进行数百次迭代），如果能以数据库专家的意见为指导，**预先选择一小部分要调整的参数和合理的数值范围**，效果会更好。我们的目标是用通过分析文本文档自动获取的信息来替代这些输入。我们将相应的问题称为自然语言处理（NLP）增强型数据库调整。

DB-BERT 从文本中提取调优提示，为特定参数推荐特定值。表 1 显示了带有来源的示例以及每个提取提示的相关正式表示。其中一些提示（第二个示例）推荐绝对值，而另一些提示（第一个和第三个示例）则推荐相对值。对于后者，将提示转化为具体的推荐值需要了解系统属性，如 RAM 的大小。一些提示（前两个示例）明确提到了参数，而另一些提示（最后一个示例）只是隐含地提到了参数。DB-BERT 可以利用表 1 中显示的所有提示。

![Table 1](https://s2.loli.net/2024/02/11/NvFeJmid4lkMoHS.png)

对于给定的文本片段，DB-BERT 使用 BERT 变换器模型的**微调版本**来解决四项任务。首先，它决定文本片段是否包含提示。其次，它将提示转化为公式，如表 1 所示。这可能需要解决隐式参数引用和相对推荐的步骤。第三，DB-BERT 可能会决定在预先定义的范围内偏离建议值，而不是完全依赖提示。最后，考虑到多个来源的提示可能相互冲突，DB-BERT 会**为提示选择权重，代表其相对重要性**

DB-BERT 并不只依赖于调优提示。相反，它使用调优提示作为基于**强化学习**的调整方法的指导。在调优过程中，DB-BERT 会不断**迭代**，直到用户定义的优化时间预算用完为止。在每次迭代中，DB-BERT 都会选择一个或多个 DBMS 配置（即参数设置）进行尝试。DB-BERT 将这些运行过程中观察到的性能（在用户定义的基准上）转化为奖励值。

在未来的迭代中，该奖励值将使用**双深度 Q-Networks 强化学习算法**来指导配置的选择。为了应用该算法，我们将数据库调整表述为一个**具有离散状态和行为的马尔可夫决策过程（MDP**）。我们将对每个提示的处理表示为一系列**决策**，确定提示类型（如相对值与绝对值）以及提示权重。为了在这些决策中利用 NLP，我们将每个决策选项与文本标签相关联。这样，DB-BERT 就能使用 BERT Transformer 比较提示文本和决策标签。我们以独立于系统和基准的方式训练 DB-BERT，然后将其应用于特定的调整任务。原则上，我们可以使用人工标注的调整文档进行训练（为与标注一致的提示翻译分配高奖励）。然而，生成这样的数据需要专家知识，很难实现众包（相比之下，标注只需要常识性知识）。相反，我们**利用数据库系统本身的（噪声）反馈**。我们假设，调整提示如果翻译正确，往往会推荐不会显著降低性能的可接受值。因此，我们在训练 DB-BERT 时，会为翻译成可接受参数设置（即 DBMS 接受该设置）的提示分配奖励。另一方面，我们会对导致不可接受的参数设置（即 DBMS 拒绝接受该设置）或显著降低简单示例工作负载性能的设置进行惩罚。

训练的结果是一个模型（即经过**微调的 BERT 模型**的约 1.1 亿个参数的权重），可用作在其他基准上调整其他数据库系统的起点。我们只知道最近有一篇关于利用文本文档进行数据库调整的论文。作者提出了一种基于**监督学习**的简单方法。该方法通过人工标注了提示翻译的调整提示进行训练。与此相反，DB-BERT 使用**未标记的文本**作为输入。输入文本无需人工预处理。与提示翻译步骤相关的选择由人工提供的文本标签（共 15 个标签）进行注释。不过，这些标签并不依赖于场景，我们在所有实验中都使用了相同的标签（表 3 显示了 15 个标签中的 5 个）。这同样适用于下文介绍的所有其他调整参数。除了人工标注开销方面的差异，先前的方法纯粹基于输入文本，没有整合任何**性能测量**，因此无法根据特定基准或指标调整推荐。

在我们的实验中，我们与后一项工作以及无输入文本数据库调整的最新方法进行了比较。我们利用通过发布带有相关关键词的谷歌查询挖掘出的大型文档集作为 DB-BERT 的文本输入。我们考虑了不同的 **benchmarks（TPC-C 和 TPC-H）**、指标（吞吐量和延迟）和数据库系统（MySQL 和 Postgres）。实验清楚地表明，通过文本分析获得的信息对 DB-BERT 大有裨益。总之，我们的原创性科学贡献如下：

- 介绍了 DB-BERT，这是一个结合自然语言文本文档和基准评估的运行时间反馈来指导数据库调整的系统。
- 描述了 DB-BERT 用于提取、优先排序、翻译、汇总和评估调整提示的机制。
- 使用多种基准、指标和数据库系统对 DB-BERT 进行了实验性评估，并与 benchmarks 进行了比较。

本文的提示组织如下。第 2 节介绍了学习和 NLP 的必要背景。然后，在第 3 节中介绍我们的问题模型和术语。第 4 节概述 DB-BERT。然后，在第 5 节中，我们将介绍 DB-BERT 如何**从文本文档中提取候选提示并对其进行优先排序**。我们在第 6 节展示 DB-BERT 如何**翻译单个提示**，在第 7 节展示 DB-BERT 如何汇总和评估提示。在第 8 节中，我们报告了实验结果，最后以第 9 节作为结束。

## Background and related work

### Pre-Trained Language Models

> 略去 LLM 介绍

### Natural Language Query Interfaces

自然语言 Query Interfaces 是预训练模型在数据库中最流行的应用。在撰写本文时，相应的方法构成了**文本到 SQL 翻译**基准的最新技术，如 WikiSQL 或 SPIDER 。将文本翻译成查询的问题与从文本中提取调整提示的问题具有某些共同特点。在这两种情况下，文本都被翻译成**形式化**的表示。然而，文本到 SQL 方法通常是将单句翻译成单个 SQL 查询，而 DB-BERT 则是从多句文本段落中提取多个调整提示。此外，DB-BERT 还必须对从多个来源获得的**相互冲突的提示进行汇总和优先排序**（这是自然语言查询界面中没有出现过的子问题）。与之前大多数有关文本到 SQL 翻译的工作不同，DB-BERT 并不假定存在**标注的训练样本**

### Reinforcement Learning

> 略去强化学习介绍

具体来说，DB-BERT 使用了双深度 Q-Networks 算法。该算法通过深度学习来估计特定状态下的行动值，使用两个独立的模型来选择行动和评估行动。强化学习已被用于数据库领域的各种问题，包括调整问题（接下来将详细讨论）。与之前的工作不同，我们将强化学习与 NLP 结合起来，以找到有前景的参数设置。更广泛地说，我们的工作与之前利用文本进行强化学习的工作相联系，特别是之前关于指令跟踪的工作。不过，之前的工作并没有考虑**性能调整**，特别是数据库调整。

### Database Tuning

之前的工作通过监督学习训练 Transformer 模型来识别包含调整提示的句子。对于被归类为调整提示的句子，它会根据简单的**启发式提取参数和数值**。这种方法仅使用文本输入，但没有运行时反馈。它从文档集中提取固定的建议集，无法适应特定的工作负载和性能指标。另一方面，DB-BERT 仅将从文本中提取的提示作为起点。它支持**更广泛的调整提示（如隐含提示**），并且在训练过程中**不需要注释调整提示**。我们在表 2 中总结了其中的一些差异，并在第 8 节中对这两种方法进行了实验比较。如今，**机器学习**已成为各种数据库优化问题的首选方法，从查询优化到物理设计决策再到数据库系统参数调整。我们要解决的是后一个问题的扩展版本，通过自然语言文本文档来扩展输入内容。

## Problem Model

我们调整数据库系统参数的配置。

定义 3.1（配置）。
每个数据库管理系统都与一组配置参数 P 相关联。用 V 表示可接受的参数值集。配置为每个参数分配一个有效值，并表示为一个函数 P ↦ V。等价地，我们将该函数表示为参数值对的集合 {⟨𝑝𝑖,𝑣𝑖⟩} for 𝑝𝑖∈ Pand 𝑣𝑖 ∈V of parameter-value pairs。配置中未引用的参数保持默认值。
我们的目标是找到优化性能的配置。传统上，我们使用以下问题模型。

定义 3.2（数据库调整）。数据库调整问题由一个元组⟨𝑏, P,V⟩ 描述。这里，𝑏 是一个基准，定义了一组查询（或事务工作量），以及需要优化的性能指标（如运行时间或吞吐量）。解决方案将从 V 中选择的用于调整的参数 P 赋值，并根据基准 𝑏 理想地优化性能。在本研究中，我们将讨论该问题模型的一个变体。

定义 3.3（NLP 增强调整）。一个 NLP 增强数据库调优实例由一个元组⟨𝑏,𝑇,𝑆⟩描述。这里，𝑏 是要优化的基准，𝑇 是包含调整提示的文本文档集合。我们的目标是利用通过自然语言分析从 𝑏 𝑇 中提取的调整提示，在考虑所有 DBMS 调整旋钮（更准确地说，我们当前的实现考虑了每个系统的所有整数、数值和布尔参数）的情况下，找到 𝑏 的最佳配置。𝑆 是一个包含数值系统属性（如内存大小或内核数量）的向量，用于将可能包含相对值建议的提示转化为具体值。我们不希望用户指定要调整的参数，也不希望用户建议参数的取值范围。我们依靠自然语言分析来确定相关参数和建议值。不过，本工作中介绍的方法假定可以访问数据库管理系统实例。通过该接口，我们可以验证提取的参数名称是否有效，以及参数类型是否在我们的范围之内（我们当前的实现考虑了整数、布尔和数值参数）。

定义 3.4（调整提示）。调优提示建议一个 DBMS 参数的值。我们将调优提示建模为一个三元组 ⟨𝑡,𝑝,𝑣⟩，其中 𝑡 是包含提示的文本片段，𝑝 是特定参数，𝑣 是 𝑡 中提到的特定值。如果 𝑝 在 𝑡 中被明确提及，我们就称该提示为显式提示，反之则为隐式提示。在伪代码中，我们使用 "𝑝 "或 "𝑡 "来指代提示 "𝑝 "的参数或文本。请注意，一个文本片段 𝑡 可能包含多个参数的建议或同一参数的多个建议值。因此，我们需要使用 𝑝 和 𝑣 来识别 𝑡 中的特定提示。𝑣 值不一定就是为 𝑝 提出的具体值。这就是为什么我们要将调整提示转化为公式的原因。

定义 3.5（翻译提示）。我们将调整提示⟨𝑡,𝑝,𝑣⟩ 转换为 𝑝 =𝑓 (𝑣,𝑆)形式的公式，其中 𝑓 是公式，𝑆 是数值系统属性（如主内存容量）的向量。我们考虑了 𝑓 (𝑣,𝑆) =𝑣 -𝑚 以及 𝑓 (𝑣,𝑆) =𝑣 -𝑆𝑖 -𝑚 类型的公式，其中 𝑆𝑖 是 𝑆 的 𝑖-th 分量，𝑚 ∈Ra 乘法器（从离散的 𝑀 乘法器集合中选取）。我们将举例说明调整提示及其转换。例 3.6. 假设 𝑆 =⟨8𝐺𝐵,4,1𝑇𝐵⟩描述了目标系统的内存容量、内核数量和磁盘空间大小。那么，𝑝 =shared_buffers 和 𝑣 =0.25 的调整提示 𝑓 (𝑣,𝑆) =𝑣 -𝑆0-1（其中 1 代表乘法器）应转换为公式 𝑓 (𝑣,𝑆) =𝑣-𝑆0-1（其中 1 代表乘法器），其值为 2 GB。

## System Overview

![DB-BERT](https://s2.loli.net/2024/02/11/RrhIUfKC2Pql319.png)

DB-BERT 搜索 DBMS 调节旋钮的设置，以根据特定基准（指定工作负载和性能指标）最大化性能。 DB-BERT 与之前的调优系统不同，它利用有关 DBMS 的文本文档来调优，例如 DBMS 手册，作为附加输入。 DB-BERT 获取要调整的基准作为输入、包含调整旋钮建议设置的文本文档集合以及描述硬件平台的数字属性（即，我们的实现期望 RAM 数量、核心数量和 磁盘空间作为输入）。后一个输入对于翻译使用相对建议的文本文档中的调整提示（例如，建议缓冲区大小占 RAM 量的百分比）是必要的。 请注意，DB-BERT 不限于与上述硬件属性相关的参数。 DB-BERT 可以处理任意参数的提示，只要将推荐值指定为文本中的绝对值即可。DB-BERT 不单独使用文本输入来确定参数设置（将其与之前的 NLP 增强数据库调优工作分开）。 相反，它利用通过对 DBMS 上的特定配置进行基准测试获得的**运行时反馈来进行调整**。 因此，DB-BERT 需要连接到 DBMS 实例。 在调优会话开始时，DB-BERT 将输入文本划分为文本片段，并尝试从每个片段中提取调优提示（图 1 中的步骤 A）。 调整提示对应于对特定参数的特定值的推荐。 从文本片段中提取提示并非易事，特别是因为参数引用可能是隐式的（即文本没有明确提及要调整的参数的名称）。 接下来，DB-BERT 确定在以下阶段考虑提示的顺序（图 1 中的步骤 B）。 理想情况下，首先考虑最重要的提示。 DB-BERT 使用**启发式方法对提示进行排序**，优先考虑经常提到的参数的提示，同时限制同一参数连续考虑的提示数量。 接下来，DB-BERT 根据调整提示迭代构建配置（即调整旋钮的值分配）。 它通过试运行在输入基准上评估这些配置。 迭代继续，直到用户中断优化或达到用户指定的优化时间限制。

在每次迭代中，DB-BERT 都会考虑**一批调优提示**（而不是整组调优提示）。它按照调整会话开始时建立的顺序考虑提示，从而首先考虑看似最重要的提示。 对于每个提示，DB-BERT 都会做出三种类型的决策。 首先，它将提示文本转换为一个简单的方程，为参数分配一个值（图 1 中的步骤 C）。 其次，在步骤 D 中，决定是否偏离推荐值（即是否将推荐值乘以常数）。 第三，它为提示**分配权重**（步骤 E）。 这些权重决定了在关于同一调谐旋钮的建议相互冲突的情况下如何确定优先级。 在处理当前批次中的所有提示后，DB-BERT 将它们聚合成一小组配置（步骤 F），**使用提示权重在不一致的推荐之间进行调解**。 它通过试运行在用户指定的基准上评估这些配置（图 1 中的步骤 G）。 DB-BERT 在调优过程中学习改进提示的翻译、调整和加权方式。 这使得 DB-BERT 能够针对当前的基准测试和平台进行专门的配置。 DB-BERT 使用**强化学习**来做出与图 1 中的步骤 C 到 E 相关的所有决策。因此，学习过程由系统试图最大化的奖励函数驱动。 对于 DB-BERT，该奖励函数基于试运行期间特定配置的性能结果。 DBMS 接受的配置（即，尝试将参数设置为特定值不会导致错误）并实现高性能，从而产生高奖励值。 根据收到的奖励，系统学习在接下来的迭代中改进其决策（图 1 中的步骤 H）。 DB-BERT 使用**深度强化学习**。 这意味着与特定选择相关的即时和未来奖励值是使用神经网络来估计的。具体来说，DB-BERT 使用预训练语言模型 BERT 作为神经网络。 由于经过预训练，该模型具有开箱即用的强大自然语言分析功能。 为了估计步骤 C 到 E 中特定选择的价值，将 BERT 应用于文本片段对。 第一个片段取自调整提示的文本，第二个片段是表示该选择的语义的文本标签（有关示例标签，请参阅第 6 节中的表 3）。 根据收到的奖励值，BERT 模型的初始权重在调优过程中（步骤 H）进行细化。

![Algorithm 1](https://s2.loli.net/2024/02/11/548Vez7XYPWvDLB.png)

算法 1 表示由 DB-BERT 以伪代码形式执行的主函数。 输入集成了用户提供的输入（如图 1 所示）以及其他参数，自动提取或在系统和基准测试中保持不变。 其中包括从 DBMS 中提取的整套整数、布尔和数字调节旋钮、𝑃、一组乘法器 𝑀（偏离文本中提出的值）、一组权重 𝑊（用于确定冲突提示之间的相对重要性） 来自不同来源），以及参数 𝑙、𝑒 和 𝑛 分别用于选择每个参数和迭代处理的提示数量、每次迭代考虑的提示总数以及每次迭代评估的配置数量。 这些参数的语义将在以下部分中更详细地描述。 算法 1 中的第 8 行实现了图 1 中的步骤 A，第 10 行实现了步骤 B。主循环迭代直到调整时间预算耗尽。 函数 Batches(𝐻𝑜,𝑒) 按照之前建立的提示顺序将提示分为最多 𝑒 大小的批次。 每次调用 RunEpisode 都会实现图 1 中的步骤 C 到 H。最后，DB-BERT 推荐出**最佳配置**。第 5 节讨论提示提取和排序。 第 6 节更详细地描述了学习过程，第 7 节概述了如何将提示聚合到配置中。

## Extracting candidate hints

![Algorithm 2](https://s2.loli.net/2024/02/11/UM5fuKGPQIziadr.png)

第一步，DB-BERT 提取候选调整提示。 根据定义 3.4，调整提示由文本片段、参数引用和值引用组成。 算法 2 描述了提取过程（如图 2 所示）。 它提取显式和隐式参数引用。 通过将文本（向量）的 BERT 编码与参数名称的 BERT 编码进行比较，选择具有最小余弦距离的参数来获得隐式引用。 我们将文本中出现的所有数字（可能与尺寸单位组合）视为潜在的价值建议。 默认情况下，我们将表示布尔标志的打开和关闭值的值 0 和 1 添加到值集中（在调整提示中通常不会明确提及打开和关闭值）。 给定文本片段的候选提示集是参数引用和值之间的笛卡尔积。 这意味着我们的候选者可能包含**错误的提示**（即未通过文本链接的参数值组合）。 将实际提示与错误提示分开的任务在翻译阶段得到解决，如下一节所述。 提取候选提示后，DB-BERT 使用算法 3 对它们进行**排序**。我们的目标是在按排序顺序考虑提示时增加找到有希望的配置的机会。 我们考虑两条经验法则。首先，我们期望在更多文档中提及重要参数。 其次，当考虑到关于同一参数的越来越多的提示时，我们**预计收益会递减**。 因此，我们优先考虑更多文档中出现的参数的提示。 然而，在切换到下一个参数之前，我们最多考虑关于同一参数的固定数量的提示。 算法 3 实现了这些高级原则。 按参数对提示进行分组后，它会迭代提示索引范围。 对于每个索引范围，它按出现次数降序迭代参数，在切换到下一个参数之前为每个参数添加 𝑙 提示（直到没有任何新提示可供为任何参数添加）

例 5.1。 图 3 说明了具有三个参数的提示排序。 蓝色矩形代表每个参数的提示。 水平宽度与提示数量成正比。 从最常提到的参数开始，我们为每个参数添加有限数量的提示。处理完最不常提及的参数（由红色箭头表示）参数 3 后，我们再次从第一个参数开始，直到不再有提示为止

![Figure 3](https://s2.loli.net/2024/02/11/9atYA7OTMkESy5W.png)

## Translating single hints

DB-BERT 将调优提示转换为算术公式（详细信息请参见定义 3.5）。 这些公式可能取决于文本中指定的值以及系统属性（例如主内存量）。 评估公式会产生调优旋钮的值建议。 对于每个调整提示，我们将翻译建模为一系列决策。 我们学习使用强化学习来翻译调整提示。 强化学习通常应用于马尔可夫决策过程（MDP），由一组状态、动作、将状态和动作对映射到新状态的转换函数以及奖励函数指定。 强化学习代理学习以观察为指导，做出最大化预期奖励的决策。 在我们的场景中，状态代表（部分指定的）算术公式。 操作指定公式的各个部分。 转换函数将部分指定的公式和操作链接到表示公式的状态，并按照操作中的指定完成。 奖励函数基于 DBMS 的反馈，惩罚导致不可接受的配置的翻译，同时奖励提高性能的更改。 我们在 6.1 节中描述环境的结构（即状态、动作、转换和奖励），在 6.2 节中描述学习代理的结构。

### Learning Environment

算法 4 实现了 DB-BERT 用于翻译**单个提示**的转换函数（伪代码接近于相应 OpenAI Gym 环境中步骤函数的实现 2）。在算法 4 中，对于固定调整提示，当前状态由部分指定的公式 (𝑓) 和变量 𝑑（下一个决策的整数 ID）来表征。 对于每个提示，我们从一个空公式 𝑓 和 𝑑 =0 开始。 我们将动作（输入 𝑎）表示为从 1 到 5 的整数。 动作的语义取决于 𝑑 的值。 对于 𝑑 =0，该操作决定当前提示是否错误（常量 NO_HINT），如果没有，则决定提示是否建议相对或绝对参数值。 相对值表示为系统属性的百分比，例如主内存或核心数量（存储在向量 𝑆 中，𝑆𝑎 代表特定向量分量）。 对于相对值，我们将 𝑓 设置为值 𝑣 与相应系统属性之间的乘积。 我们通过设置 𝑆1=1 来统一相对值和绝对值的处理（即 𝑎=1 代表绝对值）

对于 𝑑 = 1，该操作从 𝑀 中选择一个允许偏离建议值的乘数。 与之前仅提取调整提示的工作不同，这种乘法器允许 DB-BERT 适应特定的基准。 在下一节中，我们将介绍一个权衡提示的附加决策。 在这里，我们经过两次决策，已经完全明确了公式。 接下来，我们尝试将参数 𝑝 设置为公式评估结果。 如果该设置被 DBMS 拒绝，我们将直接进入结束状态（常量 END）。 这种情况会产生负奖励（激励我们的代理学习将提示转化为可接受的公式）。否则，我们评估输入基准 𝑏 的性能。 结果是奖励值。 更高的奖励与更好的绩效相关。 我们通过将性能与配置进行比较来计算奖励，以评估默认设置下的性能。 **对于 OLAP 基准（例如，TPC-H）**，我们使用运行时间的增量（按常数缩放）。 **对于 OLTP 基准（例如 TPC-C）**，我们使用吞吐量增量。 我们奖励可接受并提高性能的配置。 这两个指标与调整直接相关。 我们在应用 DB-BERT 来针对特定基准调整特定系统时使用它们。 在将 DB-BERT 应用于特定的调优任务之前，我们会执行一个训练阶段来微调 DB-BERT 的语言模型，以实现一般的提示翻译。 为了加速收敛，仅在训练期间，我们在奖励函数中添加了一个额外的组件。 该组件奖励看起来更有可能的设置，例如 因为它们与参数的默认设置具有相同的数量级。 这种启发式方法取代了先前工作中使用的手动生成的提示翻译。 图 4 说明了转换过程背后的 MDP（图 4 中的一些状态未在算法 4 中明确表示）。

![](https://s2.loli.net/2024/02/12/lTLOQERASmWyhwD.png)

### Learning Agent

DB-BERT 引入了一个**学习 agent** 来选择操作以最大化奖励。 在每种状态下，代理都会在一组离散的选项中进行选择。 每个选项都可以表示为自然语言语句。 我们可以通过将该语句与调整提示文本进行比较来找出哪个选项是正确的。 因此，我们将动作选择建模为“多项选择问题回答问题”。 预训练的语言模型可以用来解决这个问题（在我们的实现中，我们使用 BertForMultipleChoice Transformer 模型 3）。 我们在训练期间根据收到的奖励**微调模型权重**。 算法 5 显示了智能体如何根据观察来评估特定操作。 除了要评估的操作之外，输入还包括当前调整提示（调整文本 𝑡、参数 𝑝 和值 𝑣）以及当前翻译步骤（决策 𝑑）的描述。 我们将行动和决策的每个组合与标签相关联。 包含这些标签的数组通过伪代码中的常量 CHOICE_LABEL 表示。 标签是自然语言句子，代表相关选择的语义。 它包含调整提示中具体参数和值的占位符。 Instantiate 函数用具体值替换占位符。 BERT 模型使用三个输入：输入文本、将输入标记与两种输入类型之一相关联的类型标记，以及指示要考虑的标记的掩码。 在这里，我们连接提示文本和实例化标签以形成输入文本。 键入将提示文本与标签分开。 默认情况下，所有输入文本都会被考虑进行处理。 在我们的通用训练阶段发生了异常（更多详细信息，请参阅第 6.1 节）。 在这里，我们希望避免学习特定参数的名称，因为它们不能跨系统泛化。 因此，我们屏蔽所有出现的当前参数名称（函数屏蔽）。 另一方面，如果学习系统和基准测试特定配置以解决具体的调优问题，则没有理由隐藏信息。 算法 5 使用布尔标志 (MASKED_MODE) 在这两种模式之间切换。 表 3 显示了与不同操作和第一决策级别相关的标签。 在此级别，我们决定候选提示是否代表实际提示，如果是，则确定该值是相对值还是绝对值。 最后，我们通过一个例子来说明翻译。

例 6.1。 考虑调整提示 ⟨𝑡,𝑝,𝑣⟩，其中 𝑡 = “将共享缓冲区设置为 RAM 的 25%”，𝑝 = shared_buffers 和 𝑣 = 25%。 首先，代理决定提示是否有效以及是否推荐绝对值或相对值。 使用表 3 中的标签，代理根据提示文本评估替代操作。 例如，对于操作 1，代理生成输入文本“将共享缓冲区设置为 RAM 的 25%。 shared_buffers and 25%与主内存相关。”，通过类型规范分隔两个句子。 如果激活屏蔽模式，则两次出现的 shared_buffers 参数将被屏蔽。 为了做出选择，代理会在内部比较将 BERT 应用于每个可能操作的输入所产生的值。

## Aggregating Hints

最后一节描述如何翻译单个调整提示。 然而，我们经常需要**集成多个提示**（可能来自不同来源）以获得最佳性能。 DB-BERT 根据提示组创建配置。 这需要汇总来自不同来源的可能相互冲突的提示。 为了支持这一点，我们扩展了上一节中介绍的 MDP。 我们不考虑单个提示，而是考虑整批提示。 对于每个提示，我们添加一个额外的决策，将提示分配给权重。 该权重决定了将提示与其他提示聚合到配置中时的优先级。 算法 6 显示了 DB-BERT 主循环的一次迭代期间执行的完整伪代码（算法 6 由算法 1 调用）。 从强化学习的角度来看，每次迭代对应于相关 MDP 的一个情节。 每个情节都从相同的起始状态开始，代表默认配置。 因此，与默认配置相比，每集考虑的提示数量确实限制了最大更改数量。 然而，正如最近的工作所示，调整少量的调谐旋钮通常足以实现接近最佳的性能。 算法 6 获取一批候选提示作为输入。 它迭代这些提示并使用算法 4（函数 Tstep）来转换单个提示（分别确定候选提示是错误的且不应被考虑）。 我们通过指定 “-” 作为 Tstep 的基准参数来推迟基准评估。 如果成功将当前提示转换为公式（即 𝑓 ≠ −），算法 6 会分配一个权重（第 18 行）。 权重是从离散的可能性集合 𝑊 中选择的，并由学习代理（函数 ChooseAction）分配。 最后，该算法组装了一组加权调整提示 𝐻𝑤。 接下来，我们使用加权提示组装一个或多个配置进行评估。 算法 7 使用加权提示作为输入来选择和评估配置。 它迭代提示中提到的参数（从第 23 行循环到第 30 行）并选择有限数量的 𝑛 值进行尝试（𝑛 是一个调整参数）。

选择值是为了尽可能覆盖建议值（在提示中）的范围。 我们迭代地选择值（从第 26 行循环到第 29 行）。 我们希望在以下意义上尽可能接近地涵盖提示中提出的值。 给定一个比较相同参数值的距离函数 𝛿，我们的目标是最小化提示中提出的值与最接近的选定值之间的最大加权距离。 给定一组加权值 𝑉 和一组选定的配置 𝐶，函数 MaxDist 计算后一个指标。 我们贪婪地选择值，在每一步中最小化上述成本函数。 请注意，某些调整旋钮只能设置为其值范围内的特定值（例如，MySQL 的 innodb_buffer_pool_size 必须是块大小的倍数）。 我们**不能简单地平均建议值**。 例 7.1。 假设我们收集建议参数 shared_buffers 的以下值的提示：1GB，权重 1，2GB，权重 8，8GB，权重 1。当选择 1GB 时，我们获得最大加权距离 8·|2−1| = 8 GB 与值 2 GB（仅距离 1 ·|8 -1| = 7 GB 与 8 GB）。 选择 2 GB 会产生距 8 GB 值 7 GB 的最大加权距离。 选择 8 GB 会产生距值 2 GB 的最大加权距离 48 GB。 因此，我们首先选择值 2 GB。 接下来，我们选择值 8 GB 以将最大距离最小化为 1 GB。 最后，我们将每个参数的选定值组合成 𝑛 配置（第 32 行）。 Function Evaluate 在给定基准 𝑏 上评估选定的配置。 它对 DBMS 不接受的配置进行惩罚，否则根据基准性能计算奖励（我们使用第 6.1 节中介绍的奖励函数）。 函数 EvalWeighted 返回任何配置获得的最大奖励。

## Experiments

### Setup

我们比较了 MySQL 8.0 和 Postgres 13.2 的系统配置参数调整方法。 我们考虑这些系统提供的所有数字和布尔调整参数：Postgres 的 232 个参数和 MySQL 的 266 个参数。 我们使用比例因子为 1（第 8.4 节）和 10（第 8.5 节）的 TPC-H 以及比例因子为 20 的 TPCC 作为基准。 对于 TPC-C，我们使用 10 个终端，不限制到达率，预热和测量时间均为 60 秒。 除了这些参数之外，我们还使用 OLTP 基准测试中的 Postgres 和 MySQL 的默认 TPC-C 配置。 我们执行五次运行，并允许 25 分钟的调整时间（之前的工作使用相同的时间范围 ）。 所有实验均在具有 8 个 vCPU、61 GB RAM 和具有 16 GB 内存的 Tesla V100 GPU 的 p3.2xlarge EC2 实例上执行。 EC2 实例使用 Amazon Deep Learning AMI 和 Ubuntu 18.04。 我们与最近的 **DDPG++ 算法** 进行比较，作为没有 NLP 增强的调优的代表。 我们考虑调整参数的不同值范围，范围从默认值的两倍（即 𝑑/2 到 2·𝑑，其中 𝑑 是默认值）到 100。我们将这些版本表示为 DDPG2、DDPG10 和 DDPG100 以下情节。 此外，我们还与最近一篇关于 **NLP 增强数据库调优的**论文中描述的两个基线进行了比较。 在下文中，Prior-Main 表示先前工作提出的基于监督学习的主要方法。 此外，我们还与同一篇论文中描述的简单基线（表示为 Prior-Simple）进行比较

默认情况下，我们对 DB-BERT 使用以下配置参数。 DB-BERT 使用**强化学**习从一组固定的替代方案中为每个提示选择乘数值和权重。 对于所有实验，DB-BERT 从集合 {1/4,1/2,1,2,4} 中选择乘数，并从集合 {0,2,4,8,16} 中选择权重。 我们在每种情况下都使用相同数量的替代方案（五个）。 这使得使用 OpenAI Gym 框架对相关环境进行建模变得更加容易。 我们避免使用过小或过大的乘法器（如果最佳参数值在任何方向上与建议值的偏差超过四倍，则应忽略相关提示）。 权重替代集允许 DB-BERT 忽略提示（通过使用零权重），并使特定提示的重要性比其他具有非零权重的提示重要八倍。 我们将 𝑙 设置为 10，以便每个情节和参数最多允许 10 个提示。 我们总共考虑每集最多 50 个提示 (𝑒 = 50)，并评估每集的两种配置 (𝑛 = 2)。 DB-BERT 将文本文档分割成长度最多为 128 个标记的片段。 所有基线均在 Python 3.7 中实现，使用 Pytorch 1.8.1 和（对于 NLP 增强的调整基线）Huggingface Transformers 库。 DB-BERT 使用 Google 的可编程搜索引擎 API6 来检索文本文档。 此外，DB-BERT 使用自主学习库 7 中的 Double Deep Q-Networks 实现作为强化学习算法

### Tuning Text Documents

DB-BERT 附带一个脚本，可以通过 Google 搜索**检索文本文档**并将其转换为 DB-BERT 所需的输入格式。 对于以下大部分实验，我们使用通过查询“Postgresql 性能调整提示”（2021 年 4 月 11 日发布）和“MySQL 性能调整提示”（2021 年 4 月 15 日发布）检索的两个文档集合。 我们将这两个查询的前 100 个 Google 结果包含到相应的文档集合中（Postgres 总共占 1.3 MB 文本，MySQL 总共占 2.4 MB 文本）。 结果多种多样，涵盖博客条目、论坛讨论（例如，数据库管理员 Stack Exchange8）以及两个数据库系统的在线手册。 下面我们将 Postgres Pg100 的文档集合和 MySQL Ms100 的文档集合称为。 图 5 显示了这些文档集合中参数提及和建议值分配的分布，这些文档集合是通过 DB-BERT 的候选提示提取机制生成的（参见第 5 节）。 显然，文档和参数上的提示分布是不均匀的。 对于这两个数据库系统，在多个文档中提到的参数很少，而大多数参数仅在单个文档中提到。 同样，多个来源提出了一些任务。 另一方面，大多数价值分配仅提出一次。 表 4 显示了 Postgres 和 MySQL 最常提到的参数。 其中与缓冲区大小相关的参数（例如，Postgres 的共享缓冲区和 MySQL 的 innodb_buffer_pool_size）尤为突出。 除此之外，与并行性（例如 max_parallel_workers_per_gather）或日志记录（例如 max_wal_size）相关的参数也经常被提及。

### Training

其中两种比较算法，即 DB-BERT 和 Prior-Main，在运行前使用训练。 Prior-Main 使用自然语言调整提示（用相关公式注释）作为训练数据。 我们使用与之前的工作相同的训练样本和训练参数。 与后一篇论文中的实验设置一致，我们应用在 Postgres 样本上训练的 Prior-Main 来调优 MySQL，并应用在 MySQL 样本上训练的 Prior-Main 来调优 Postgres。 目标是证明 **NLP 增强的数据库调优不需要特定于系统的带注释的示例**。 Prior-Main 不支持从固定文档集合中提取特定于基准测试的调整提示，如果使用相同的文档集合来调整多个基准测试，则这是一个缺点。 为了至少允许一定程度的可变性，我们为五个基准运行中的每一个单独训练 Prior-Main 模型。 这导致每次运行中的提取略有不同。 在第 8.1 节中概述的平台上训练 Prior-Main 对于 MySQL 样本需要 417 秒，对于 Postgres 样本需要 393 秒。

DB-BERT **不使用带注释的调优提示进行训练**。 相反，它在训练阶段使用数据库系统**本身进行运行时反馈**。 与 Prior-Main 类似，我们在 Pg100 上训练 DB-BERT 来调整 MySQL，在 Ms100 上训练 DB-BERT 来调整 Postgres。 我们在训练期间激活 mask mode（参见第 6 节），这意味着参数名称被屏蔽。 这避免了学习系统特定的参数名称（这在我们的实验设置中无用），并将注意力集中在调整提示的句子结构上。 DB-BERT 的奖励信号（参见第 6 节和第 7 节）结合了根据调整提示成功更改参数值（意味着相应值有效）和获得的性能的奖励。 为了衡量性能，我们使用一个综合数据库，其中包含两个表，其中两列包含从 1 到 1,000,000 的连续数字。 我们使用简单的计数聚合查询通过相等谓词连接两个表。 对性能的奖励按比例缩小了 100 倍，以避免专门针对此人为基准（它仅用于惩罚特别糟糕的配置，例如将缓冲池大小设置为最小值）。 最后，我们为设置与默认设置相同数量级的参数值添加一个小的奖励奖金（假设与默认值的极端偏差是可能的，但可能性较小）。 DB-BERT 的训练从具有 1.1 亿个参数的 BERT 基础模型 [3] 开始。 所有模型参数均在训练期间调整。 我们在 Pg100 上训练 DB-BERT 5,000 次迭代，在 Ms100 上训练 DB-BERT 10,000 次迭代（由于该集合中的提示数量较多）。 Pg100 的训练时间为 43 分钟，Ms100 的训练时间为 84 分钟。 图 6 显示了 Pg100 的进度与训练步骤数的函数关系

### Comparison with Baselines

我们将 DB-BERT 与 TPC-H（见图 7）和 TPC-C（见图 8）的基线进行比较。 我们每次运行都会调整 Postgres 和 MySQL 25 分钟。 我们使用**吞吐量**作为 TPC-C 的优化指标和 TPC-H 的执行时间。 我们将找到的最佳配置（y 轴）的性能显示为优化时间（x 轴）的函数。 在这些图中和下面的图中，我们报告了算术平均值以及五次运行的第 20 个和第 80 个百分位数（使用误差线显示百分位数）。

DDPG++ 是一种基于强化学习的数据库调优方法。 它被证明与各种其他最先进的调整方法具有竞争力。 然而，之前的论文评估了 DDPG++ 的数十个调整参数，并为每个调整会话分配 150 次迭代。 在这里，我们考虑了数百个参数进行调整，并**只允许很少迭代的调整时间范围**。 显然，在分配的时间范围内，DDPG++ 没有找到与 DB-BERT 质量相当的解决方案。 特别是对于 TPC-H，DDPG++ 经常尝试显着降低性能的参数更改（例如，优化器成本常量的更改触发不同的连接顺序）。 因此，对于 DDPG++，找到的最佳配置的性能几乎保持不变（接近通过初始迭代之前尝试的默认配置实现的性能）。 DDPG++ 可以受益于指定在调整期间要考虑的特定参数值范围。 例如，与默认设置相比，将缓冲池大小增加一个数量级通常是有益的。 然而，对于优化器成本常量（例如 Postgres 中的 random_page_cost），这样做是危险的。 我们的目标是**表明此类输入可以部分地被从文本中自动挖掘的信息所替代**。

Prior-Simple 和 Prior-Main 是两个最相关的基线，因为两者都使用调整文本作为输入，类似于 DB-BERT。 PriorSimple 使用朴素启发式进行翻译。 应用这种启发式方法很快，并且 Prior-Simple 通常是返回结果的第一个基线。 但是，它仅从 Pg100 中提取将 checkpoint_completion_target 设置为 0.9 的建议，而没有从 Ms100 中提取建议。 因此，它不会比默认配置有所改进。 Prior-Main 的性能明显更好。 由于训练中的微小差异，不同运行中的提取有所不同，从而导致较高的方差。 例如，对于 Pg100，Prior-Main 能够提取一个调整提示，建议在五次运行中有两次将共享缓冲区设置为主内存的 25%。 这可以显着提高性能，特别是对于 TPC-H。 然而，平均性能明显低于最佳性能。 由于 Prior-Main 在聚合调整提示之前对文档集合中的所有句子进行分类，因此其运行时间明显高于 Prior-Simple。

DB-BERT 在**调优时间和结果质量之间实现了有吸引力的权**衡。 与 DDPG++ 不同，它使用调整文本作为输入，可以快速识别最相关的参数和候选值。 与 Prior-Simple 和 Prior-Main 相比，它平均找到明显更好的解决方案。 特别是对于 MySQL，Prior-Main 通常无法找到质量相当的解决方案。 此外，与 DB-BERT 产生接近最优解的时间（即，与 DB-BERT 最终最优解的百分之一以内）相比，Prior-Main 分析所有文档所花费的时间通常要高出两到三倍。 ）。 表 5 和表 6 显示了 DB-BERT 在调优 Postgres 时发现的配置。 尽管从同一文档集合中提取提示，DB-BERT 仍能够找到特定于基准的配置

![](https://s2.loli.net/2024/02/12/8hxuFtfbJcN62Bg.png)

### Further Analysis

我们研究不同因素对调优性能的影响。 首先，我们将 DB-BERT 与图 9 中的两个简化变体进行比较。我们与按文档顺序处理提示的 DB-BERT 变体进行比较（而不是按照第 5 节中的描述对提示进行优先级排序）。 此外，我们还与不考虑隐式提示的变体进行比较（即，仅显式提及参数名称的提示）。 显然，这两种简化都会降低 TPC-H 的调整性能。 考虑文档顺序中的提示可以防止 DB-BERT 首先调整最相关的参数。 **丢弃隐式提示会减少可用提示的总数**。

接下来，我们**研究输入文本的影响**。 我们用一篇博客文章 9 替换了包含数百个通用调整提示的 Pg100。 这篇文章描述了如何专门针对 TPC-H 调整 Postgres。 图 5 比较了所有 NLP 增强调整基线的不同输入文档的性能。 虽然 Prior-Simple 的性能不会随着输入文本的变化而变化，但当我们切换到较小的文档时，Prior-Main 的性能会下降。 Prior-**大型文档集合**的主要好处是冗余提示可以部分弥补不精确的提取。 对于较小的输入文档，它不会提取任何提示。 然而，DB-BERT 受益于**更专业的调整提示**。 使用特定于基准的输入文本，它可以更快地收敛到接近最佳的解决方案，并最终找到稍微更好的解决方案（与表 5 相比，使用更高的共享缓冲区参数值，如博客条目中所建议的）。

![](https://s2.loli.net/2024/02/12/mpVTvZBbhSENC74.png)

最后，我们扩大数据大小。 图 11 报告了比例因子为 10 的 TPC-H 的结果（并使用 TPC-H 特定调整文本）10。 与显示缩放因子 1 的结果的图 10 相比，DB-BERT 需要**更长的时间才能找到接近最优的解决方案**。 这是预期的，因为每个基准评估的运行时间较长会减少每个时间单位的 DB-BERT 迭代次数。 与其他基线相比，DB-BERT 再次找到了明显更好的解决方案。

![](https://s2.loli.net/2024/02/12/MBvydKXpUurhRCL.png)

## Conclusion and Outlook

本文提出了 DB-BERT，这是一个数据库调优系统，可以从文本文档中提取调优提示。 我们的实验表明，此类提示可以带来明显更好的调整结果。 在未来的工作中，我们将考虑更加多样化的调优目标。 目前，DB-BERT **仅限于优化可轻松测量的指标，例如延迟或吞吐量**。 然而，还有其他一些难以衡量的重要指标。 例如，如果愿意接受数据丢失的小风险，许多参数（例如 Postgres 中的 fsync 参数）可以提高性能。 数据库手册通常包含详细说明此类风险的警告。 我们计划扩展 DB-BERT，以提取难以从手册中测量的指标信息。 因此，它可以支持用户找到最大化性能的参数设置，同时遵守其他指标的约束。
